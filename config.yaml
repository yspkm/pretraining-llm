train:
  max_len_seq: 1024 
  num_layers: 32
  dim_model: 2304 
  dim_hidden: 9216 
  num_heads: 36
  prob_dropout: 0.50
  batch_size: 20 
  val_interval: 1000 
  total_steps: 550000
  # 4409721
  val_steps: 50 
  grad_accum_steps: 25
  lr_peak: 0.000025 # 휴리스틱
  weight_decay: 0.01
  warmup_steps: 2000
  balance: [11, 11, 11, 1]
  devices: ["cuda:0", "cuda:1", "cuda:2", "cuda:3"]
  random_seed: 0

wandb:
  project_name: "AutoLLaMA"
  model_name: "llama"

prep_data:
  num_proc: 64
  encoding: "gpt2"
  train_file: "train.bin"
  val_file: "val.bin"
  eval_file: "eval.bin"
  num_shards: 1024
  dataset_name: "openwebtext"
  split_ratio: [0.999, 0.0005, 0.0005]
  seed: 2357
  dtype: "np.uint16"