train:
  max_len_seq: 1024 
  num_layers: 14 
  dim_model: 1024 
  dim_hidden: 4096 
  num_heads: 16
  prob_dropout: 0.20
  batch_size: 12
  val_interval: 1000 
  total_steps: 550000
  # 4409721
  val_steps: 50 
  grad_accum_steps: 40
  lr_peak: 0.00025 
  weight_decay: 0.01
  warmup_steps: 2000
  random_seed: 2357

wandb:
  project_name: "AutoLLaMAProject"
  model_name: "gpt"

prep_data:
  num_proc: 64
  encoding: "gpt2"
  train_file: "train.bin"
  val_file: "val.bin"
  eval_file: "eval.bin"
  num_shards: 1024
  dataset_name: "openwebtext"
  split_ratio: [0.999, 0.0005, 0.0005]
  seed: 2357
  dtype: "np.uint16"
